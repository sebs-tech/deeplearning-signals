# Keras optimizers comparison and training analysis

## Definition 
In Keras, an open-source neural network library written in Python, optimizers are algorithms or methods used to adjust the weights of a neural network during training to minimize the loss function. Essentially, they help in finding the best parameters for the model to make accurate predictions.

1. SGD (Stochastic Gradient Descent): The simplest form of optimization algorithm. It updates the weights using the gradient of the loss function with respect to the weights, often with a small learning rate. Variants include momentum and Nesterov accelerated gradient (NAG).

